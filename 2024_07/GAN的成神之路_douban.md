## 我们已经完成的工作：

### 1.在转绘前分割背景和人像。

借鉴于以太转绘流程，我们在现实->动漫转绘前也加入了人像分割环节。

意义:

* 训练上，我们分开训练背景和人像GAN网络更容易拟合，且拟合效果更好。

* 推理时我们可以搭配不同风格的背景和图像，比如新海诚风格的人物配上油画风格的背景，或者实现背景风格变化（季节变化，夜晚白天变化）。

* 支持输出空白背景（用户可以自己给它加上想要的背景，需要一点点AE的知识）：

  ![Snipaste_2024-06-30_07-44-20](https://image.baidu.com/search/down?url=https://img3.doubanio.com/view/photo/l/public/p2910018152.webp)

* 支持单独对人物或者背景迁移是可行的，类似这样（背景是不变的）：

  ![Snipaste_2024-06-30_07-42-31](https://image.baidu.com/search/down?url=https://img3.doubanio.com/view/photo/l/public/p2910018153.webp)

### 2.对UGATIT的模型架构和训练进行了优化：

* 借鉴于Style-GAN，将UGATIT的判别器优化为多尺度（不同分辨率，不同空间）判别器。同时也保持local，global的判别器初始化。local主要看细节（高分辨率），global主要看整体（低分辨率）。
* 借鉴于Projected-GANs，我们不用每次都从0开始训练GAN模型，我们可以实现部分“迁移学习”，而避免传统GAN迁移时的模式崩溃。
* 在生成数据集和清洗上，我们设计了一个留出法思想的数据保留方式。源数据通过api利用Stable-Diffusion生成，结合上面的人像分割我们可以无人监控的情况下自主生成想要的数据集。

## 需要完成的工作：



### 1. FastAPI的部署和手机小程序的开发。

我们之前利用python的gradio库简单设计了一个WebUI,它可以直接托管于modelscope,huging-face这样的网站。

但是出于便携性的考虑，用户可能主要是使用手机，则需要考虑植入比如微信程序之中。

具备条件：API可以直接从gradio那边迁移，以及UI方面是小组内是有人专门负责的，UI这边并不是我做的。

### 2.一种GAN通用的，基于时序（前后帧图像）的监督器。

这是在生成视频帧序列的时候发现的问题——虽然画面没有剧烈抖动变化，但是随机噪点让视频整体看上去有点像胶片电影的那种感觉，不是很干净。

以前在接触Deep-LSTMs的时候，关于Time Series(时序）的想法可以被迁移到这里。

#### **主要功能1：随机噪点的抑制**

**消除GAN的浮动噪点（这是随机噪点）**。学校开的计算机视觉课程给我科普了一下传统方法，这也是思路来源：

![Snipaste_2024-06-30_08-46-43](https://image.baidu.com/search/down?url=https://img9.doubanio.com/view/photo/l/public/p2910018155.webp)

对于随机噪点，我们是可以通过多张图像求均值来算出来减少随机噪点在图像上的视觉表现。

而我们的思路是:

* **前后相邻两帧之间的差异通常很小**

* **前后两帧的差值是位移（一个连续的轮廓），但对于我们生成的GAN图像来说，还带了不连续的随机噪点。**
* 我们去掉连续的轮廓，两帧之间的随机噪点传给监督器。并且**让监督器进行一个随机噪点的抑制，训练时，让减法得到的值趋向极小值，如果趋于0，就变成了没有噪点浮动。**我们理想得到的上图redisuals为纯黑色背景下带了一个清晰的人物轮廓。（上面residuals是差值，其实有很多白点但是看不清。）

#### **主要功能2：时序（在图像上应该是前后两张图像的空间分布）一致性的检验**

 这个是处理一些相邻帧局部有的突变，比如即使加了多尺度判别器，生成不同帧之间人物的眼睛还是偶尔会闪烁，不是很严重，但是影响观感。

这个思路来源是AI补帧。

如果我们的差值包含了清晰轮廓，噪点外，还包含了例如半个完整的眼睛（这种的），判别依据是人物轮廓内出现了大面积差值，这通常不是噪声，而是前后两帧局部突变。【当然这边的差值不能用简单的减法了，而是**用类似transformer的position embedding传递的突变信息**，而对于帧序列整体信息，我们用注意力来捕获。】

这边的算法就不方便直接透露了，而且也仍然在构建中。

这种情况下，我们会考虑丢弃突变帧，并且根据突变帧前后，最后补帧。

这里也需要有个简单科普。

**一拍三：**

同一幅画面连续播放三帧。换句话说，在每秒24帧的动画中，每幅画面会重复播放三次，这样每秒就只需要绘制8幅画面。这种方法既可以节省动画制作的时间和成本，又能在一定程度上保持动画的流畅性。

也就是说，动漫的帧率和3d视觉视觉游戏的帧率其实不大一样，关键帧有就可以。而且关键帧极少。不用太害怕丢弃突变帧导致的后果。
