# 强化学习基础_Q-Learning:

notebook位于:[85.介绍-QLearning算法.ipynb](https://github.com/MrXnneHang/AI-By-Doing/blob/master/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/85.%E4%BB%8B%E7%BB%8D-QLearning%E7%AE%97%E6%B3%95.ipynb)<br>

这个仓库里的系列原作者都不是我，我只是来学的，原作者的课程我挂在仓库了。赛博菩萨，如果时间充足，整套课程学下来甚至比不少408出身的老师要专业和全面。<br>

我可能不会按照顺序来，需求排在第一位。<br>

代码可以自己拿着notebook放在[google colab](https://colab.research.google.com/drive/1qPHiK0tJRodIpEoQZvfwFRqUpjQqLHG2)跑。<br>

我这里就讲讲思路。<br>

我们需要三张图。<br>

![image-20241022164841766](https://fastly.jsdelivr.net/gh/MrXnneHang/blog_img/BlogHosting/img/24/10/image-20241022164841766.png)


![alt text](https://fastly.jsdelivr.net/gh/MrXnneHang/blog_img/BlogHosting/img/24/10/image-2.png)

![alt text](https://fastly.jsdelivr.net/gh/MrXnneHang/blog_img/BlogHosting/img/24/10/image-4.png)

接下来我就直接称他们为A,B,C了。<br>

A介绍游戏和游戏规则。<br>

游戏和规则：<br>

* 狮子要吃火腿，但是不能踩地雷。<br>
* 不一定要最短路径，只要不踩地雷，吃到火腿就行，允许反复横跳。<br>
* 地雷和火腿的位置不会改变。<br>

实际上我们更希望这种学习能够认知地雷和火腿，然后自适应火腿和地雷的相对位置。<br>

但是实际上并没有那么复杂,因为接下来要介绍的Q-Table原理也比较简单。主要是改变一下认知，就像以前神经网络刚刚杀进来的时候，正常程序员逻辑是绕不过来的，为啥能那样呢，但实际上再一看，强化学习，至少Q-Learning的本质和神经网络那么像，数学层面上都是利用和调整一些bias和weight来修正我们的函数来最终实现近拟合或者局部最优。<br>

不过神经网络的方法是梯度下降和反向传播，强化学习这边，就有意思多了，可操作性也更强。<br>

接下来看B和C。<br>

B是我们Q-Learning的算法核心,也就是Q-Table。<br>

对于我们这个游戏来说，Q-Table反映的是在每一个格子上下左右移动的Q-Value，也就是我们认为的奖励或者惩罚的积攒(通常需要一个学习率，还有对多次游戏进行一个平均。)。<br>

奖励和惩罚是我们自定义的，比如说，我们定义吃到火腿为正（奖励），踩到地雷为负（惩罚）。

再看我们训练后的Q-Table。<br>

```cmd
Index = x * 4 + y (0<=x<=3,0<=y<=3)
```
很容易猜到每个(x,y)都应该对应Q-Table上面的一个行。<br>

对于15(3,3),火腿在它的左边，所以left是接近10的。<br>

还有几个10 和 -10的情况也是类似。<br>

所以对于Q-Table的本质，实际上是记录所有可能的情况，以及所有情况下的对应策略的结果。<br>

通过Q-Value的迭代把坏的结果迭代成-10,把好的结果迭代成10。<br>

最终形成的是，我总是避免踩到地雷，我总是优先吃到火腿，如果它在我附件，但如果目标离我很远，实际上它会有点随机行事，也不能算随机，应该是固定套路。<br>

因为最终value确定下来后，会有差距，它会根据Q-Value更大的固定行动，最终找出一种解法。<br>

如果是一个五子棋的对手的话，大概是很差劲的，它对自己要输或者要赢很敏感，比如，四子和三子它会堵上，避免它绝对会输的情况。但让它攻的话就会很随机，它会固定一个方向走，而且很容易被打断棋路，再陷入另一种随机。<br>

但如果训练得相当好的话，假设它经历过所有残局，那么它应该会立于不败之地，你很难让他输。<br>

当然我前面说的它攻的棋路会被我们打断，也不一定，如果就连我打断它棋路的方式也已经被它学习过了，那么实际上在走向失败的是我。<br>

虽然五子棋的一个残局非常庞大，但我打算先来写一个三子棋。<br>

在那之前，我可以把狮子的地图扩大一些，比如5x5或者6x6。然后再次观察。<br>

好吧已经差不多废了。我把地图扩大后(6x6),目标(火腿/陷阱)在里面占比更小，它已经会来回徘徊了。<br>

因为缺少导向性，可能是我的惩罚机制定义得不够好，但是即使再怎么修补，也很难弥补空矩阵的情况，我们理想情况下，我们希望的是每一种情况的下每一种决策都被well trainning。<br>

但是实际上是这种情况：<br>

![alt text](https://fastly.jsdelivr.net/gh/MrXnneHang/blog_img/BlogHosting/img/24/10/image-5.png)

大部分值是空的，如果我们试图用一个Q-Table来训练五子棋的自己博弈，它会固定一个棋路，或者一直拖延。如果两个Q-Table,大概也是一直拖延。<br>

加上五子棋需要对每一种残局构成都进行统计，决策的数量也高达width x height。<br>

训练不好，根本训练不好。即使写出来Q-Table,也无法训练出想要的效果。<br>

这个算法效率低的原因在于，它把每一种情况之间的因果关联性给忽略了，试图遍历所有情况，但是实际上情况约多，越难以在所有情况里找出一个解法，也更难找出更多解法。<br>

我不太适合想算法，我只是来学的。<br>

不过确实，Q-Table很好地表现了一种放养和机器自己学的思路，尽管它很吃力和艰难。<br>