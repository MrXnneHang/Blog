# 自然语言处理复习。



## 中文分词方法：

### 基于规则

FMM ( Forward Maxlen matching )

依赖词典。

一个长度maxlen的窗口在文本中从左向右移动和匹配。

找到单词就切分出来，没找到就单独切分一个字出来。然后接着填充到maxlen，重复匹配。

还有逆向和双向。

逆向相反，双向就是综合考虑两个的结果。取想要的一个。

### 基于统计

#### N-gram

1-gram： 统计词的出现频率 : $frac{P(wi),ALL}$

2-gram： 给定前一个词，后一个词出现的频率 : $frac{P(wi-1,wi),P(wi-1)}$

3-gram：给定前两个词，第三个词出现的频率 : $frac{P(wi-2,wi-1,wi),P(wi-2,wi-1)}$

...

具体怎么用？

混合用还是综合考虑？

没有那么复杂。

都是给定一个确定的n，然后进行模型训练。

另外，因为这个是给予概率的，那么这个也不需要反复迭代训练，只需要看一遍数据，就能求出来各个分词的组合结果。

对于正常来说，n=2,n=3,n=4都是不错的选择。

#### HMM 隐马尔可夫。

相对于n-gram来说，它更人性化，而且考虑到了自然语言本身的关联性。

## Word Embedding

### Skip-gram

windows size = 2

![image-20250106181043712](C:\Users\Zhouyuan\AppData\Roaming\Typora\typora-user-images\image-20250106181043712.png)

win_size = 3

![image-20250106181118666](C:\Users\Zhouyuan\AppData\Roaming\Typora\typora-user-images\image-20250106181118666.png)