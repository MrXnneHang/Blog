```python
"""
1. 初始化棋盘和历史记录
2. 创建 QLearningAgent 实例
3. 游戏循环，智能体选择动作、更新棋盘状态、检查赢家、更新 Q 值
4. 记录每一步的棋局和动作，完成一局后重置并开始下一局
"""

import random
import numpy as np

class TicTacToeBoard:
    """
    三子棋棋盘类，用于表示和管理三子棋的棋盘状态
    """
    def __init__(self):
        """
        初始化棋盘为3x3的空数组，空位用0表示，玩家1用1表示，玩家2用-1表示
        """
        self.board = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]

    def reset(self):
        """
        重置棋盘，将棋盘的所有位置重置为0（空位置）
        """
        self.board = [[0, 0, 0] for _ in range(3)]

    def display(self):
        """
        打印当前棋盘的状态，方便可视化。X代表玩家1，O代表玩家2，空位置用空格表示
        """
        for row in self.board:
            print(' | '.join(['X' if x == 1 else 'O' if x == -1 else ' ' for x in row]))
            print('-' * 9)

    def is_full(self):
        """
        检查棋盘是否已经被填满，没有可用的位置。如果棋盘满了，返回True，否则返回False
        """
        return all(all(cell != 0 for cell in row) for row in self.board)

    def check_winner(self):
        """
        检查是否有玩家获胜。检查每行、每列、对角线是否有相同的符号。
        返回1表示玩家1获胜，-1表示玩家2获胜，0表示没有赢家
        """
        # 检查每一行
        for row in self.board:
            if sum(row) == 3:
                return 1  # 玩家1获胜
            elif sum(row) == -3:
                return -1  # 玩家2获胜

        # 检查每一列
        for col in range(3):
            if sum(self.board[row][col] for row in range(3)) == 3:
                return 1  # 玩家1获胜
            elif sum(self.board[row][col] for row in range(3)) == -3:
                return -1  # 玩家2获胜

        # 检查主对角线
        if self.board[0][0] + self.board[1][1] + self.board[2][2] == 3:
            return 1  # 玩家1获胜
        elif self.board[0][0] + self.board[1][1] + self.board[2][2] == -3:
            return -1  # 玩家2获胜

        # 检查副对角线
        if self.board[0][2] + self.board[1][1] + self.board[2][0] == 3:
            return 1  # 玩家1获胜
        elif self.board[0][2] + self.board[1][1] + self.board[2][0] == -3:
            return -1  # 玩家2获胜

        return 0  # 没有赢家

class GameHistory:
    """
    棋谱记录类，用于保存每局游戏的状态和动作
    """
    def __init__(self):
        """
        初始化一个空的历史记录，用于存储每一步的棋局状态和相应动作
        """
        self.history = []

    def add_step(self, board_state, action):
        """
        将当前的棋盘状态和动作添加到历史记录中

        参数:
        board_state: 当前的棋盘状态
        action: 当前的动作
        """
        self.history.append((board_state, action))

    def reset(self):
        """
        重置历史记录，清空所有已存储的记录
        """
        self.history = []

class QLearningAgent:
    """
    基于Q-learning算法的智能体，用于学习如何在三子棋中进行决策
    """
    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):
        """
        初始化Q-learning算法的参数

        参数:
        alpha: 学习率，用于控制Q值更新的步长
        gamma: 折扣因子，用于平衡即时奖励和未来奖励
        epsilon: 探索概率，用于控制智能体随机探索的频率
        """
        self.q_table = {}  # Q表，保存每个状态对应的动作的Q值
        self.alpha = alpha  # 学习率
        self.gamma = gamma  # 折扣因子
        self.epsilon = epsilon  # 探索概率

    def get_state_key(self, board):
        """
        将棋盘状态转换为可哈希的元组形式，作为Q表的键

        参数:
        board: 当前棋盘的状态（二维数组）
        
        返回:
        棋盘状态的元组表示（作为Q表中的键）
        """
        return tuple(map(tuple, board))  # 将二维数组转为不可变的元组

    def choose_action(self, board):
        """
        根据当前的棋盘状态选择动作。使用ε-贪婪策略：有epsilon的概率进行探索，其余情况选择当前最优动作。

        参数:
        board: 当前的棋盘状态（二维数组）

        返回:
        选择的动作（0-8的数字，表示棋盘的九个位置）
        """
        state_key = self.get_state_key(board)
        # 如果Q表中没有当前状态，则初始化该状态下所有动作的Q值为0
        if state_key not in self.q_table:
            self.q_table[state_key] = [0] * 9  # 9个位置，对应9个可能的动作

        if random.random() < self.epsilon:  # 探索
            return random.choice(self.available_actions(board))
        else:  # 利用，选择Q值最高的动作
            return np.argmax(self.q_table[state_key])

    def update_q_value(self, board, action, reward, next_board):
        """
        根据Q-learning算法更新Q值

        参数:
        board: 当前状态的棋盘
        action: 在当前状态下执行的动作
        reward: 执行动作后获得的奖励
        next_board: 执行动作后得到的下一步棋盘状态
        """
        state_key = self.get_state_key(board)  # 当前状态
        next_state_key = self.get_state_key(next_board)  # 下一状态

        # 如果Q表中没有下一状态，则初始化该状态下所有动作的Q值为0
        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = [0] * 9

        # Q-learning 更新公式
        self.q_table[state_key][action] += self.alpha * (
            reward + self.gamma * max(self.q_table[next_state_key]) - self.q_table[state_key][action]
        )

    def available_actions(self, board):
        """
        获取当前棋盘状态下的可用动作（空位）

        参数:
        board: 当前的棋盘状态（二维数组）

        返回:
        一个列表，包含所有可用动作（即棋盘上空位的索引）
        """
        # 将二维棋盘转换为一维形式，返回空位的索引（即值为0的位置）
        return [i for i in range(9) if board[i // 3][i % 3] == 0]

```

