##  国内外的热门研究:

现在主要主要的研究都聚焦于扩散模型上，以及语义结合的多模态模型。

但是这些都会受到一点影响，那就是扩散模型的扩散本质和扩散步数让它会有一种前后帧关联信息的丢失，也就是说它在视频流的生成里面，是容易闪烁的，虽然有很多插件和工作流抑制闪烁，比如controlnet用轮廓限制，fastblend在生成后进行一个帧和帧之间的混合重计算。但它都是不治本的。

而传统GAN模型，弊端很明显，训练难，迁移难。但是,U-GA-TIT的图像循环一致性检验在图像闪烁上面起到了非常大的效果，即使没有其他因素作用，视频流的输出中也只会存在浮动噪点和局部突变。而文末我们也提出了一种专注于视频流的时序一致性检验的监督器方法来进行一个画面稳定和消噪。无疑，它在视频流风格转绘上不管是推理成本还是画面稳定性都是比扩散模型更优的选择。

## 研究内容:

配图的详细说明位于:

[http://xnnehang.top/blog/38](http://xnnehang.top/blog/38)

### 已完成：

### 1.设计工作流:人像分割->背景和人像分开选择迁移->合并背景和人像的工作流。

### 2.优化了模型架构和训练：

* 借鉴于Style-GAN，将UGATIT的判别器优化为多尺度（不同分辨率，不同空间）判别器。同时也保持local，global的判别器初始化。local主要看细节（高分辨率），global主要看整体（低分辨率）。
* 借鉴于Projected-GANs，我们不用每次都从0开始训练GAN模型，我们可以实现部分“迁移学习”，而避免传统GAN迁移时的模式崩溃。
* 在生成数据集和清洗上，我们设计了一个留出法思想的数据保留方式。源数据通过api利用Stable-Diffusion生成，结合上面的人像分割我们可以无人监控的情况下自主生成想要的数据集。

### 待完成：

#### **一种视频流GAN通用的，基于时序（前后帧图像）的监督器。**

**主要功能1：GAN生成图像时随机噪点的抑制**

随机噪点在单张图像上不明显，但是在视频的多帧表现上会有种像是胶片电影一样的颗粒感和不干净的干净。

思路来源：多张同样的图像带有不同随机噪点之间求均值可以在视觉效果上抑制噪音。

实现逻辑：

* **前后相邻两帧之间的差异通常很小**

* **前后两帧的差值是位移（一个连续的轮廓），但对于我们生成的GAN图像来说，还带了不连续的随机噪点。**
* 我们去掉连续的轮廓，两帧之间的随机噪点传给监督器。并且**让监督器进行一个随机噪点的抑制，训练时，让减法得到的值趋向极小值，如果趋于0，就变成了没有噪点浮动。**

**主要功能2：时序（在图像上应该是前后两张图像的空间分布）一致性的检验**

这个是处理一些相邻帧局部有的突变，比如即使加了多尺度判别器，生成不同帧之间人物的眼睛还是偶尔会闪烁，不是很严重，但是影响观感。

这个思路来源是AI补帧。

如果我们的差值包含了清晰轮廓，噪点外，还包含了例如半个完整的眼睛（这种的），判别依据是人物轮廓内出现了大面积差值，这通常不是噪声，而是前后两帧局部突变。【当然这边的差值不能用简单的减法了，而是**用类似transformer的position embedding传递的突变信息**，而对于帧序列整体信息，我们用注意力来捕获。】

根据相似帧之间补出理想帧，并且根据理想帧和突变帧之间的距离作为loss。

这可以理解为是一种保证前后帧位移不变性的一种额外网络，也可以被迁移到其他GAN系的模型中。

这边的算法就不方便直接透露了，而且也仍然在构建中。





## 创新内容：

### 速度的创新，多帧一致性的创新：

* 推理速度：（对比stable-diffusion，运行环境为4060ti-16G）

推理转绘600x800的图像，U-GA-TIT的lite模型可以做到一秒4帧，large模型一秒1.5帧。

而stabe-diffusion的图生图在没有加任何插件的情况下，同等分辨率一帧通常需要13s~17s。

* 训练速度:  (对比其他GAN系列和U-GA-TIT项目论文)

对比原论文：我们的数据集是由Stable diffusion生成且一一对应的，无监督训练的拟合效果和拟合速度大大提高，同时验证集上无明显模型退化现象。

对比其他GAN:借鉴于Projected-GANs可以让我们迁移地训练模型而不必每次从零开始，起点更高。

* 风格转绘的循环一致性不再只是单帧而适用于多帧：（对比原论文，以及同样具有循环一致性检验环节的风格转绘GAN）

我们在设计一种基于多帧序列突变点检测的监督器。这个监督器的模型参数是单独保存的，只在视频转绘的时候使用。可以抑制GAN多帧中的随机噪点，以及局部突变。



### 功能性和趣味性的创新:

这是衍生出来的一个子项目：将png或者jpg平涂动漫人物图像转换成可面补的多图层Live2d模型。

配图的详细说明位于:（该板块建议配图食用）

[http://xnnehang.top/blog/37](http://xnnehang.top/blog/37)

#### **已完成：**

我们借鉴了一种简单的，将.psd（PS分图层的工程项目，可以被解析成图像）的多图层图像的分图层提取和渲染，进行面补的算法形成类似live2d（虚拟2d或者2.5d角色，可以用面部捕捉控制）模型的算法。

原理:

* **分图层渲染，并且将所有图层贴在一个曲面上**，渲染出一种伪立体感。

同时这个曲面转动，就可以让动画人物的头部转动起来。增大曲率，就可以增大可转动的角度。

* 面部捕捉的参数用于控制的图层的变形。

  对于变形区域，比如嘴巴，我们是通过图层变形控制的。并且将嘴巴复制了一份（上嘴唇和下嘴唇），当不说话时重合，说话时则控制变形。

#### **待完成：**

- 眼睛眨动还需隐藏图层，暂时并未完成，嘴巴还未填色。
- 而目前主要的任务则是**如何将png单图层拆分成理想多图层**

将png单图层拆分成理想多图层的解法：

**首先，一张平涂动漫画风的图像，通常情况下，颜色是可以数出来的，有限的，而且轮廓是清晰的硬边。**

实现逻辑：

- 先对图像进行一个规格化，将图像相邻像素之间变化不大的全部标准化。对于我们这个psd最终剩下的颜色区域不超过10种。
- 将每个颜色区域切分并且输出到我们的分类模型。（微调后的resnet18表现得就很好),分类输出class为hair，eye，body，mouth,others
- 将相同的class拼接到一起，并且将连通的区域用canny检测轮廓进行连通。
- 这样我们就能够input:png和jpg图像 -> output：分图层的图像。
- 之后就可以继续我们上述面捕live2d步骤。
